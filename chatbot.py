import os
import logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from config import (
    GOOGLE_API_KEY,
    MODEL_CONFIG,
    VECTORSTORE_CONFIG,
    SYSTEM_PROMPT
)

# Configuration de la cl√© API Google
os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

# Initialisation du mod√®le
logger.info("Initializing model...")
model = ChatGoogleGenerativeAI(
    model=MODEL_CONFIG["model_name"],
    google_api_key=GOOGLE_API_KEY,
    temperature=MODEL_CONFIG["temperature"],
    max_output_tokens=MODEL_CONFIG["max_output_tokens"],
    top_p=MODEL_CONFIG["top_p"],
    convert_system_message_to_human=True
)
logger.info("Model initialized successfully!")

# Initialisation du vector store
logger.info("Initializing vector store...")

from langchain_google_genai import GoogleGenerativeAIEmbeddings

# Initialisation des embeddings
embedding = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    task_type="retrieval_document"
)

# Configuration du vector store
logger.info("Initializing vector store...")
vectorstore = Chroma(
    persist_directory="vectorstore_livrable01",
    embedding_function=embedding,
    collection_name="chatbot"
)
logger.info("Application initialized successfully!")

# Configuration de la m√©moire
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# Configuration du prompt
prompt = PromptTemplate(
    template=SYSTEM_PROMPT,
    input_variables=["context", "question", "chat_history"]
)

# Cr√©ation de la cha√Æne de conversation
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=model,
    retriever=vectorstore.as_retriever(
        search_kwargs={"k": VECTORSTORE_CONFIG["k_nearest_neighbors"]}
    ),
    memory=memory,
    combine_docs_chain_kwargs={"prompt": prompt}
)

def format_response(response):
    """Formate la r√©ponse pour l'affichage."""
    print(f"Structure de la r√©ponse re√ßue: {type(response)}")
    print(f"Contenu de la r√©ponse: {response}")
    
    if isinstance(response, dict):
        if "answer" in response:
            return response["answer"]
        elif "text" in response:
            return response["text"]
        else:
            print(f"Cl√©s disponibles dans la r√©ponse: {response.keys()}")
            return str(response)
    return str(response)

def chat_with_bot():
    """
    Fonction principale pour interagir avec le chatbot.
    G√®re la conversation et affiche les r√©ponses avec les sources.
    """
    print("ü§ñ Bienvenue ! Je suis votre assistant RAG sp√©cialis√© dans l'analyse de documents.")
    print("üí° Vous pouvez me poser des questions sur les documents index√©s.")
    print("üí° Pour quitter, tapez 'quit'")
    
    while True:
        try:
            # R√©cup√©ration de la question de l'utilisateur
            question = input("\nüë§ Votre question : ")
            
            if question.lower() == 'quit':
                print("üëã Au revoir !")
                break
            
            # G√©n√©ration de la r√©ponse
            response = qa_chain({"question": question})
            
            # Formatage et affichage de la r√©ponse
            formatted_response = format_response(response)
            
            print("\nü§ñ R√©ponse :")
            print(formatted_response)
            
        except KeyboardInterrupt:
            print("\nüëã Au revoir !")
            break
        except Exception as e:
            print(f"‚ùå Une erreur est survenue : {str(e)}")
            print("üí° Essayez de reformuler votre question ou de v√©rifier votre connexion internet.")
            continue

if __name__ == "__main__":
    chat_with_bot() 